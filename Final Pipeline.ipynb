{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihan\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nihan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>final_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mark the spy</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My account says its been temporarily locked. ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Put on your tin foil hats to deflect the mind ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Im so sick and tired of us working our tails ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content  final_label\n",
       "0                                       Mark the spy            4\n",
       "1                                              Title            4\n",
       "2  My account says its been temporarily locked. ...            3\n",
       "3  Put on your tin foil hats to deflect the mind ...            1\n",
       "4  Im so sick and tired of us working our tails ...            4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Dataset/filtered_reviews.csv')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_label\n",
       "4    2846\n",
       "2    2428\n",
       "1     567\n",
       "3     434\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['final_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (2.32.3)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (3.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.2.0->nlpaug) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.22.0->nlpaug) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->gdown>=4.0.0->nlpaug) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    # Tokenization using TextBlob\n",
    "    tokens = TextBlob(text).words\n",
    "    \n",
    "    # Stemming using NLTK PorterStemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokens = [porter_stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization using NLTK WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "\n",
    "df['processed_review_content'] = df['review_content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Instantiate augmentation techniques\n",
    "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment data for minority classes\n",
    "augmented_data = []\n",
    "for label in [1, 3]:\n",
    "    minority_data = df[df['final_label'] == label]['processed_review_content']\n",
    "    for text in minority_data:\n",
    "        augmented_text = augmenter.augment(text)\n",
    "        augmented_data.append({'processed_review_content': augmented_text, 'final_label': label})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and augmented data\n",
    "balanced_data = pd.concat([df, pd.DataFrame(augmented_data)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_label\n",
      "4    2846\n",
      "2    2428\n",
      "1    1134\n",
      "3     868\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the class distribution\n",
    "print(balanced_data['final_label'].value_counts())\n",
    "\n",
    "# Now, use balanced_data for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_label\n",
      "4    2846\n",
      "1    2835\n",
      "3    2604\n",
      "2    2428\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Instantiate augmentation techniques\n",
    "augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Augment data for minority classes\n",
    "augmented_data = []\n",
    "for label in [1, 3]:\n",
    "    minority_data = df[df['final_label'] == label]['processed_review_content']\n",
    "    # Determine the augmentation factor based on the ratio of minority to majority class\n",
    "    augmentation_factor = int((df['final_label'].value_counts().max() / len(minority_data)) - 1)\n",
    "    for text in minority_data:\n",
    "        for _ in range(augmentation_factor):\n",
    "            augmented_text = augmenter.augment(text)\n",
    "            augmented_data.append({'processed_review_content': augmented_text, 'final_label': label})\n",
    "\n",
    "# Combine original and augmented data\n",
    "balanced_data = pd.concat([df, pd.DataFrame(augmented_data)])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Check the class distribution\n",
    "print(balanced_data['final_label'].value_counts())\n",
    "\n",
    "# Now, use balanced_data for training your model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data.to_csv('balanced_data_with_value_counts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_content</th>\n",
       "      <th>final_label</th>\n",
       "      <th>processed_review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>[open facebook account earli 2000 instantli fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[say your gon sodium get facebook first time y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When they updated this app, suddenly it beca...</td>\n",
       "      <td>4</td>\n",
       "      <td>updat app suddenli becam difficult access phot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(?????????)Imagine this: you want to use Twitt...</td>\n",
       "      <td>4</td>\n",
       "      <td>imagin want use twitter youd rather data colle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[woke facebook woke much detector trail spi li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      review_content  final_label  \\\n",
       "0                                                NaN            3   \n",
       "1                                                NaN            1   \n",
       "2  When they updated this app, suddenly it beca...            4   \n",
       "3  (?????????)Imagine this: you want to use Twitt...            4   \n",
       "4                                                NaN            1   \n",
       "\n",
       "                            processed_review_content  \n",
       "0  [open facebook account earli 2000 instantli fe...  \n",
       "1  [say your gon sodium get facebook first time y...  \n",
       "2  updat app suddenli becam difficult access phot...  \n",
       "3  imagin want use twitter youd rather data colle...  \n",
       "4  [woke facebook woke much detector trail spi li...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Dataset/balanced_data_with_value_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.3-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (1.26.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp310-cp310-win_amd64.whl (299 kB)\n",
      "   ---------------------------------------- 0.0/300.0 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/300.0 kB 640.0 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 92.2/300.0 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 143.4/300.0 kB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 245.8/300.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 276.5/300.0 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 300.0/300.0 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Validation Accuracy (SVM): 0.749708284714119\n",
      "Classification Report (SVM - Validation):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.93      0.93       455\n",
      "           2       0.54      0.46      0.49       402\n",
      "           3       0.95      0.96      0.95       405\n",
      "           4       0.58      0.64      0.61       452\n",
      "\n",
      "    accuracy                           0.75      1714\n",
      "   macro avg       0.75      0.75      0.75      1714\n",
      "weighted avg       0.75      0.75      0.75      1714\n",
      "\n",
      "Test Accuracy (SVM): 0.7634157722818479\n",
      "Classification Report (SVM - Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.93      0.92       579\n",
      "           2       0.58      0.48      0.52       505\n",
      "           3       0.95      0.97      0.96       506\n",
      "           4       0.59      0.66      0.62       553\n",
      "\n",
      "    accuracy                           0.76      2143\n",
      "   macro avg       0.76      0.76      0.76      2143\n",
      "weighted avg       0.76      0.76      0.76      2143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define SVM model\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the model on the TF-IDF transformed training set with the best hyperparameters\n",
    "best_svm = SVC(**best_params, random_state=42)\n",
    "best_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the TF-IDF transformed validation set\n",
    "val_preds_svm = best_svm.predict(X_val_tfidf)\n",
    "val_accuracy_svm = accuracy_score(y_val, val_preds_svm)\n",
    "print(\"Validation Accuracy (SVM):\", val_accuracy_svm)\n",
    "print(\"Classification Report (SVM - Validation):\\n\", classification_report(y_val, val_preds_svm))\n",
    "\n",
    "# Evaluate the model on the TF-IDF transformed test set\n",
    "test_preds_svm = best_svm.predict(X_test_tfidf)\n",
    "test_accuracy_svm = accuracy_score(y_test, test_preds_svm)\n",
    "print(\"Test Accuracy (SVM):\", test_accuracy_svm)\n",
    "print(\"Classification Report (SVM - Test):\\n\", classification_report(y_test, test_preds_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Naive Bayes): 0.5915985997666278\n",
      "Classification Report (Naive Bayes - Validation):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.67      0.71       455\n",
      "           2       0.48      0.24      0.32       402\n",
      "           3       0.77      0.71      0.74       405\n",
      "           4       0.44      0.71      0.54       452\n",
      "\n",
      "    accuracy                           0.59      1714\n",
      "   macro avg       0.61      0.58      0.58      1714\n",
      "weighted avg       0.61      0.59      0.58      1714\n",
      "\n",
      "Test Accuracy (Naive Bayes): 0.5725618292113859\n",
      "Classification Report (Naive Bayes - Test):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.63      0.67       579\n",
      "           2       0.49      0.26      0.34       505\n",
      "           3       0.72      0.67      0.70       506\n",
      "           4       0.43      0.71      0.54       553\n",
      "\n",
      "    accuracy                           0.57      2143\n",
      "   macro avg       0.59      0.57      0.56      2143\n",
      "weighted avg       0.59      0.57      0.56      2143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train the model on the TF-IDF transformed training set\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model on the TF-IDF transformed validation set\n",
    "val_preds_nb = nb.predict(X_val_tfidf)\n",
    "val_accuracy_nb = accuracy_score(y_val, val_preds_nb)\n",
    "print(\"Validation Accuracy (Naive Bayes):\", val_accuracy_nb)\n",
    "print(\"Classification Report (Naive Bayes - Validation):\\n\", classification_report(y_val, val_preds_nb))\n",
    "\n",
    "# Evaluate the model on the TF-IDF transformed test set\n",
    "test_preds_nb = nb.predict(X_test_tfidf)\n",
    "test_accuracy_nb = accuracy_score(y_test, test_preds_nb)\n",
    "print(\"Test Accuracy (Naive Bayes):\", test_accuracy_nb)\n",
    "print(\"Classification Report (Naive Bayes - Test):\\n\", classification_report(y_test, test_preds_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Perform Grid Search CV\u001b[39;00m\n\u001b[0;32m     23\u001b[0m grid_search_xgb \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mxgb_model, param_grid\u001b[38;5;241m=\u001b[39mxgb_param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[43mgrid_search_xgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_adjusted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Best hyperparameters\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters for XGBoost:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search_xgb\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1698\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1699\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Ensure your class labels start from 0\n",
    "y_train_adjusted = y_train - 1\n",
    "y_val_adjusted = y_val - 1\n",
    "y_test_adjusted = y_test - 1\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search CV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=xgb_param_grid, cv=3, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_tfidf, y_train_adjusted)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_preds_xgb = grid_search_xgb.predict(X_val_tfidf)\n",
    "val_accuracy_xgb = accuracy_score(y_val_adjusted, val_preds_xgb)\n",
    "print(\"Validation Accuracy (XGBoost):\", val_accuracy_xgb)\n",
    "print(\"Validation Classification Report (XGBoost):\")\n",
    "print(classification_report(y_val_adjusted, val_preds_xgb, target_names=['class 0', 'class 1', 'class 2', 'class 3']))\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_preds_xgb = grid_search_xgb.predict(X_test_tfidf)\n",
    "test_accuracy_xgb = accuracy_score(y_test_adjusted, test_preds_xgb)\n",
    "print(\"Test Accuracy (XGBoost):\", test_accuracy_xgb)\n",
    "print(\"Test Classification Report (XGBoost):\")\n",
    "print(classification_report(y_test_adjusted, test_preds_xgb, target_names=['class 0', 'class 1', 'class 2', 'class 3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.30.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (14.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.1.3)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.1 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 61.4/542.1 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 61.4/542.1 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 122.9/542.1 kB 1.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 122.9/542.1 kB 1.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 122.9/542.1 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------- ---------------------- 225.3/542.1 kB 919.0 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 245.8/542.1 kB 754.9 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 266.2/542.1 kB 820.5 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 266.2/542.1 kB 820.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 378.9/542.1 kB 843.6 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 378.9/542.1 kB 843.6 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 409.6/542.1 kB 775.5 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 430.1/542.1 kB 726.4 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 430.1/542.1 kB 726.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 542.1/542.1 kB 792.7 kB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 30.7/116.3 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------- ------------------------- 41.0/116.3 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------- ------------------------- 41.0/116.3 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 116.3/116.3 kB 757.2 kB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 112.6/370.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 112.6/370.7 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 122.9/370.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 122.9/370.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 122.9/370.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 122.9/370.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- ------------------ 194.6/370.7 kB 692.9 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 194.6/370.7 kB 692.9 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 194.6/370.7 kB 692.9 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 307.2/370.7 kB 791.9 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 307.2/370.7 kB 791.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  368.6/370.7 kB 764.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- 370.7/370.7 kB 743.2 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 122.9/401.7 kB 7.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 194.6/401.7 kB 2.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 204.8/401.7 kB 2.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 204.8/401.7 kB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 358.4/401.7 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 358.4/401.7 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  399.4/401.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 401.7/401.7 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB ? eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 71.7/134.8 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 71.7/134.8 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 71.7/134.8 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 71.7/134.8 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- 134.8/134.8 kB 613.7 kB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 41.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 183.5 kB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/76.4 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 10.2/76.4 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 41.0/76.4 kB 281.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 76.4/76.4 kB 529.3 kB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, requests, pyarrow-hotfix, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.2 dill-0.3.8 frozenlist-1.4.1 huggingface-hub-0.23.3 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 requests-2.32.3 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade datasets transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[supplication least give u way app transpositi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[facebook sell information freak disabl track ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[could give zero star would. four month ago ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[account pay back hack long ago itÂ  ridicul ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[mark z censor conserv sell data]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  [supplication least give u way app transpositi...      1\n",
       "1  [facebook sell information freak disabl track ...      1\n",
       "2  [could give zero star would. four month ago ac...      1\n",
       "3  [account pay back hack long ago itÂ  ridicul ...      3\n",
       "4                  [mark z censor conserv sell data]      3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = balanced_data['processed_review_content']\n",
    "y = balanced_data['final_label']\n",
    "\n",
    "data = pd.DataFrame({'review': X, 'label': y})\n",
    "\n",
    "# Check the dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b89fc55008e4a40a1de9c57152df996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029112666ff74677aab0c5400c45848e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3373d6165288422c942ed6690701ba47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f222884fa464b0dbb51f41c2d8832e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1048686c46b4fe0b745cca7c35a873c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ad6d82bea04a1996f6756038d81894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f854254314ccdaef2797fbdedff50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1607 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Convert non-string values in the 'review' column to strings\n",
    "train_df['review'] = train_df['review'].astype(str)\n",
    "val_df['review'] = val_df['review'].astype(str)\n",
    "test_df['review'] = test_df['review'].astype(str)\n",
    "\n",
    "# Convert pandas dataframe to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "val_dataset = val_dataset.rename_column('label', 'labels')\n",
    "test_dataset = test_dataset.rename_column('label', 'labels')\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.31.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.30.2)\n",
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers[torch])\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers[torch])\n",
      "  Using cached safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nihan\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nihan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/287.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/287.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/287.4 kB ? eta -:--:--\n",
      "   ------------ -------------------------- 92.2/287.4 kB 581.0 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/287.4 kB 656.4 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 112.6/287.4 kB 656.4 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 235.5/287.4 kB 801.7 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 235.5/287.4 kB 801.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 287.4/287.4 kB 770.0 kB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 7.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 1.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.7/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.2/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.2 MB/s eta 0:00:00\n",
      "Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.1\n",
      "    Uninstalling safetensors-0.3.1:\n",
      "      Successfully uninstalled safetensors-0.3.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.2\n",
      "    Uninstalling transformers-4.30.2:\n",
      "      Successfully uninstalled transformers-4.30.2\n",
      "Successfully installed safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~afetensors'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Nihan\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.2.5 requires transformers<4.31.0,>=3.4.0, but you have transformers 4.41.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch] accelerate -U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n",
      "0.31.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(accelerate.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
